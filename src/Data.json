{
  "Project": {
    "waxmuseum": {
      "title": "Wax Museum",
      "dateStart": "Sept 2019",
      "dataEnd": "Dec 2019",
      "blurb": "This project uses laptop footage to turn off the laptop screen when I'm not looking at it.",
      "img": "0.png",
      "links": [
        "https://github.com/SinclairHudson/wax-museum",
        "https://app.wandb.ai/sinclairhudson/waxmuseum?workspace=user-sinclairhudson"
      ],
      "tags": [
        "Computer Vision",
        "PyTorch",
        "Weights & Biases",
        "OpenCV"
      ],
      "gallery": [
        {
          "src": "0.png",
          "width": "480",
          "height": "480"
        },
        {
          "src": "1.png",
          "width": "640",
          "height": "480"
        }
      ],
      "video": null,
      "url": "/project/waxmuseum",
      "intro": "Two Problems. I don't know PyTorch, and my laptop battery keeps dying during lecture. The latter has easier solutions. I could lower my brightness, reduce non-essential usage, or remember to charge my laptop. However, I'm a bit of an AI geek and I had an unreasonable yet simple solution in mind: turn off the screen when I'm not looking at it, and turn it back on when I am. I just need a simple convolutional net to perform binary classification. This will stretch my battery life further, and give me a chance to learn PyTorch. Problems solved.",
      "learned": [
        "Weights & Biases analytics",
        "Tuning hyperparameters",
        "PyTorch"
      ],
      "custom": [
        {
          "header": "Weights and Biases",
          "body": "Weights and Biases is a new analytics platform for training neural networks. It's like TensorBoard, but better in functionality and easier to use. It allows you to compare runs effortlessly, sweep through hyperparameters, and display custom metrics. I used Weights and Biases in this project to find optimal hyperparameters like the learning rate, and to display performance metrics so I could pick the best model. I also displayed a bunch of activation maps, because they're really cool. :)"
        }
      ],
      "howItWorks": "This network is a simple convolutional one, with about 4 layers. I use OpenCV to get frames from the webcam, and then run them through the network. I then use the past 3 readings to smooth the output. If the network says I'm looking away, then I dim the screen through the command line. Otherwise, I keep the screen lit. It's only slightly more complex than an if statement in an infinite loop."
    },
    "vm": {
      "title": "VM",
      "date-start": "Nov 2019",
      "date-end": "Dec 2019",
      "blurb": "For the final project of CS246E, a partner and I built a version of VIM in C++.",
      "img": "0.png",
      "links": [
      ],
      "tags": [
        "C++",
        "Systems Design",
        "Design Patterns"
      ],
      "gallery": [
        {
          "src": "0.png",
          "width": "5",
          "height": "5"
        },
        {
          "src": "VM.png",
          "width": "734",
          "height": "496"
        }
      ],
      "intro": "For the final project of CS246E, we were instructed to implement vim in C++. It was a massive object-oriented design project. In total, we had to support 56 commands, including macros and unbounded undos. My partner and I spent weeks planning out our design, making sure every command could be implemented easily. We used many design patterns and SOLID principles to create an excellent design that made implementation straightforward. In the end our complete coverage of the specification, as well as a few bonus features, earned us a grade of 104%.",
      "video": null,
      "url": "/project/vm",
      "learned": [
        "Applying SOLID Principles",
        "Writing a very detailed design report, along with a formal UML",
        "Pair programming with a partner",
        "Applying Object-Oriented design patterns",
        "Project planning and software design"
      ],
      "custom": [],
      "howItWorks": "In the end, our project centered around the Command design pattern. Essentially, all commands were represented as Command objects. There was a command parser class, which was in charge of converting keyboard input into Command objects. Keep in mind that commands in VIM can be repeated, like ‘6fc’. The commands were stored on a stack structure to keep undos clean. We had a class that handled the text being manipulated, and file management. We also had a class for displaying the content, which followed the Observer design pattern to update when the contents or cursor changed."
    },
    "conduct": {
      "title": "Conduct",
      "dateStart": "June 2019",
      "dataEnd": "August 2019",
      "blurb": "This project allows me to control audio playback on my computer by making gestures with my hands. It uses a YOLOv3 algorithm on webcam input.",
      "img": "0.jpg",
      "links": [],
      "tags": [
        "TensorFlow 2.0",
        "Computer Vision",
        "YOLOv3",
        "OpenLabeling",
        "OpenCV"
      ],
      "gallery": [
        {
          "src": "0.jpg",
          "width": "320",
          "height": "320"
        },
        {
          "src": "1.jpg",
          "width": "1347",
          "height": "973"
        },
        {
          "src": "2.jpg",
          "width": "1514",
          "height": "497"
        }
      ],
      "video": "https://www.youtube.com/embed/CPZNZpvM5Xc",
      "url": "/project/conduct",
      "intro": "I’m always listening to music while I work, and I thought it would be cool to control my music through very intuitive hand gestures. This is one case where voice recognition gets unreliable; if you’re blasting music, the microphone has trouble picking up your commands. So, you use your hands to control the computer, like a conductor directs a symphony.",
      "learned": [
        "YOLOv3",
        "Tuning hyperparameters",
        "Learning rate decay and warmup epochs to help a network converge",
        "Processing and displaying outputs using OpenCV"
      ],
      "custom": [
        {
          "header": "What's Next?",
          "body": "There are a lot of things that could be added to this project fairly easily. I could mess around with the hyperparameters a little bit more to improve accuracy, I could photograph and tag more data, and I could add more gestures, like one for skip track. Top priority would be to increase the size of the dataset. I created and tagged the dataset myself, in one afternoon, in the same room, in the same clothes. Needless to say, the system isn’t really robust, since it’s only trained on 1500 similar frames. Sigh… tagging computer vision data is such a bore, but that’s the only surefire way to improve my model."
        }
      ],
      "howItWorks": "The system is pretty easy to understand on a high level. There’s an infinite loop, taking images from the webcam that’s attached to my wall, and feeding them to a neural network. The YOLOv3 network is nice and fast, with enough accuracy to pick up my hands and classify them. The network not only detects where the hands are, but classifies them as one of 6 hand states: palm, back, flat, vertical, fist, and neutral. After the network has given its predictions, it’s just a matter of simple logic to perform the audio modifications through terminal commands."
    },
    "ajax": {
      "title": "AJAX ASR",
      "dateStart": "Sept 2018",
      "dataEnd": "Feb 2019",
      "blurb": "Voice Recognition system built with Google Tensorflow.",
      "img": "0.jpg",
      "links": [
        "https://github.com/SinclairHudson/AJAX"
      ],
      "tags": [
        "TensorFlow",
        "NLP",
        "NumPy",
        "Tensorboard",
        "Keras",
        "LSTM"
      ],
      "gallery": [
        {
          "src": "0.jpg",
          "width": "320",
          "height": "320"
        },
        {
          "src": "1.jpg",
          "width": "1527",
          "height": "472"
        },
        {
          "src": "2.jpg",
          "width": "3264",
          "height": "4352"
        },
        {
          "src": "3.jpg",
          "width": "361",
          "height": "286"
        }
      ],
      "video": null,
      "url": "/project/ajax",
      "intro": "This is my longest TensorFlow project, which all started with a frustration with a horrible voice recognition library. I decided to give it a shot myself, using Google TensorFlow Maching Learning. Currently, I have some parts working with a sub-optimal accuracy, and I'm looking into different models and methods to achieve great voice recognition. You can find all my code on my GitHub, right here.",
      "learned": [
        "Implementing and testing sequence classification neural networks",
        "Google Tensorflow, NumPy, Pyaudio, and many other small libraries",
        "Patience",
        "A Research, Build, Test cycle to complete a project"
      ],
      "custom": [],
      "howItWorks": "Currently, I've only trained a word classifier working, capable at differentiating 18 different words apart, with about 92% accuracy. The idea was to determine where the word breaks were, partition the sound file into words, and then classify from there. However, with a low classification accuracy, I'm currently looking into a new approach, using a sequence to sequence LSTM network to turn the sound byte into phenomes first. The good news is that I already have the data; I wrote a python script that automatically labels and records my voice, making data collection easy."
    },
    "tripwire": {
      "title": "Tripwire",
      "dateStart": "June 2019",
      "dataEnd": "June 2019",
      "blurb": "An app built in React Native that triggers alarms when in proximity to GPS coordinates.",
      "img": "0.jpg",
      "links": [
        "https://github.com/SinclairHudson/tripwire",
        "https://devpost.com/software/tripwire"
      ],
      "tags": [
        "React Native",
        "EngHack 2019",
        "UI/UX"
      ],
      "video": null,
      "url": "/project/tripwire",
      "intro": "This was a project I created with three of my friends for EngHack 2019. We decided to tackle a light problem: we were all missing our public transit stops because we were reading, sleeping, or on our phones. Obviously, we needed a way to alert us when we got close to a bus stop or specific location. We were all new to React Native, so it was pretty rough, but we all had a good time.",
      "learned": [
        "React Native, specifically navigation in React Native",
        "Advanced Git VC to get four devs working together on one project",
        "Rapid Prototyping in a team"
      ],
      "gallery": [
        {
          "src": "1.jpg",
          "width": "1080",
          "height": "2340"
        },
        {
          "src": "3.jpg",
          "width": "1080",
          "height": "2340"
        },
        {
          "src": "2.jpg",
          "width": "1080",
          "height": "2340"
        },
        {
          "src": "0.jpg",
          "width": "320",
          "height": "320"
        },
        {
          "src": "4.jpg",
          "width": "3456",
          "height": "4608"
        },
        {
          "src": "5.jpg",
          "width": "4608",
          "height": "3456"
        }
      ],
      "custom": [],
      "howItWorks": "The app uses the GPS in the phone for its main functionality. The user can set certain coordinates as waypoints, and specify what kind of alarm should trigger when the phone came within a specified radius of the point."
    },
    "synviz": {
      "title": "Synviz",
      "dateStart": "Jan 2020",
      "dataEnd": "Jan 2020",
      "github": "https://github.com/SinclairHudson/Synviz",
      "wandb": null,
      "blurb": "Placing 3rd in UofTHacks VII, Synviz is a wearable device that reads lips.",
      "img": "prototype.jpg",
      "links": [
        "https://devpost.com/software/synviz",
        "https://github.com/SinclairHudson/Synviz"
      ],
      "tags": [
        "TensorFlow",
        "GCP",
        "Raspberry Pi",
        "OpenCV"
      ],
      "gallery": [
        {
          "src": "prototype.jpg",
          "width": "320",
          "height": "320"
        },
        {
          "src": "pipeline.jpg",
          "width": "1347",
          "height": "973"
        },
        {
          "src": "team_photo.jpg",
          "width": "1514",
          "height": "497"
        }
      ],
      "video": null,
      "url": "/project/synviz",
      "intro": "My friend Martin came across a cool paper, \"Deep Lip Reading: a comparison of models and an online application\" by Afouras et al. In the paper, the authors try multiple pipelines, trying to ",
      "learned": [
        "Creating a deep learning pipeline, deploying a custom model.",
        "Building a very compact prototype.",
        "Pitching well at a large hackathon"
      ],
      "custom": [
        {
          "header": "What's Next?",
          "body": "There are a lot of things that could be added to this project fairly easily. I could mess around with the hyperparameters a little bit more to improve accuracy, I could photograph and tag more data, and I could add more gestures, like one for skip track. Top priority would be to increase the size of the dataset. I created and tagged the dataset myself, in one afternoon, in the same room, in the same clothes. Needless to say, the system isn’t really robust, since it’s only trained on 1500 similar frames. Sigh… tagging computer vision data is such a bore, but that’s the only surefire way to improve my model."
        }
      ],
      "howItWorks": "The system is pretty easy to understand on a high level. There’s an infinite loop, taking images from the webcam that’s attached to my wall, and feeding them to a neural network. The YOLOv3 network is nice and fast, with enough accuracy to pick up my hands and classify them. The network not only detects where the hands are, but classifies them as one of 6 hand states: palm, back, flat, vertical, fist, and neutral. After the network has given its predictions, it’s just a matter of simple logic to perform the audio modifications through terminal commands."
    },
    "website": {
      "title": "This Website",
      "date-start": "Dec 2018",
      "date-end": "Present",
      "blurb": "This website is built by myself from scratch, designed in Illustrator, and coded using React.",
      "img": "0.jpg",
      "links": [
        "https://github.com/SinclairHudson/sinclairhudson.github.io"

      ],
      "tags": [
        "React",
        "Frontend",
        "Design",
        "Illustrator"
      ],
      "gallery": [
        {
          "src": "0.jpg",
          "width": "5",
          "height": "5"
        },
        {
          "src": "1.jpg",
          "width": "6215",
          "height": "5432"
        }
      ],
      "intro": "I designed and coded this website entirely by myself in 2018, but it was in pure HTML and CSS. I used Illustrator to lay everything out and get a feel for the design. The project got way to large to maintain, and editing the HTML files for each individual page was a pain. So, I purchased a book on the React Framework, and spun this current website over a few weekends, learning as I went.",
      "video": null,
      "url": "/project/website",
      "learned": [
        "React",
        "Creating dynamic websites, in which things move, slide, and interact",
        "Project Structuring for large websites"
      ],
      "custom": [],
      "howItWorks": "This website is built with create-react-app, and is highly abstracted. All the data is in one file, and all the styling and structure is in React Components. All the pages are procedurally generated, so if I want to add a project, I just have to put the information into the data file. I used libraries for the image gallery and the Parallax banners, but everything else I coded myself. While a lot of people have told me that I shouldn't re-invent the wheel, I find value in learning how to build these things by myself."
    },
    "speaker": {
      "title": "DIY Speaker",
      "date-start": "June 2018",
      "date-end": "July 2018",
      "blurb": "I built a speaker and amplifier circuit from scratch using components found at a hardware store, soldering it all together and testing it with an oscilliscope",
      "img": "0.jpg",
      "tags": [
        "Hardware",
        "Soldering",
        "3D Modelling",
        "3D Printing",
        "Oscilliscope"
      ],
      "gallery": [
        {
          "src": "0.jpg",
          "width": "5",
          "height": "5"
        },
        {
          "src": "1.jpg",
          "width": "3456",
          "height": "4608"
        },
        {
          "src": "2.jpg",
          "width": "1286",
          "height": "747"
        },
        {
          "src": "3.jpg",
          "width": "3300",
          "height": "2444"
        },
        {
          "src": "4.jpg",
          "width": "3264",
          "height": "3264"
        },
        {
          "src": "5.jpg",
          "width": "1013",
          "height": "824"
        },
        {
          "src": "6.jpg",
          "width": "3264",
          "height": "4352"
        }
      ],
      "intro": "During my Grade 12 Year, I had the wonderful opportunity to work with an ex-Hydro One electrical engineer, James Whatley, in my Computer Engineering 12 Class. We learned and then built voltage regulation circuits, amperage regulations circuits, full-wave rectifiers, but we were short on time at the end of the year. Eager to learn more, I talked a bit with my instructor and we agreed to meet for a few days in the early summer to work on another project: the portable speaker.",
      "url": "/project/speaker",
      "video": "https://www.youtube.com/embed/BCQs8cIpOGc",
      "learned": [
        "Operational Amplifier circuits",
        "Using an oscilliscope to debug physical circuits",
        "Audio signal processing, filtering, and amplifying",
        "Current and Voltage limiting circuits",
        "Soldering complex circuits from schematics"
      ],
      "custom": [],
      "howItWorks": "Currently, I've only trained a word classifier working, capable at differentiating 18 different words apart, with about 92% accuracy. The idea was to determine where the word breaks were, partition the sound file into words, and then classify from there. However, with a low classification accuracy, I'm currently looking into a new approach, using a sequence to sequence LSTM network to turn the sound byte into phenomes first. The good news is that I already have the data; I wrote a python script that automatically labels and records my voice, making data collection easy."
    },
    "custompc": {
      "title": "Custom PC",
      "dateStart": "June 2016",
      "dataEnd": "Jan 2018",
      "blurb": "I built and now maintain my own PC. You could say I'm a PC enthusiast.",
      "img": "4.jpg",
      "tags": [
        "Hardware",
        "Dual-Booting"
      ],
      "video": null,
      "url": "/project/custompc",
      "intro": "Over the last few years, I have slowly built and upgraded my personal computer. The project started in grade 10 when I dissected a few old computers from my father’s workplace. From there, I slowly purchased and salvaged parts for the the next two years, arriving at this system. Along the way, I learned a great deal about how PCs work, and how to repair them. I have also helped a few friends out with building their own systems. My current build dual-boots Windows and Ubuntu.",
      "learned": [
        "How all components of a computer are installed, and how they all work together",
        "How to design a cost-effective build that has all the features I need",
        "Configuring BIOS settings, and dual booting",
        "File system permissions and formatting"
      ],
      "gallery": [
        {
          "src": "1.jpg",
          "width": "3377",
          "height": "4099"
        },
        {
          "src": "3.jpg",
          "width": "540",
          "height": "960"
        },
        {
          "src": "2.jpg",
          "width": "3455",
          "height": "4245"
        },
        {
          "src": "0.jpg",
          "width": "3456",
          "height": "4063"
        }
      ],
      "custom": [],
      "howItWorks": "I don't like being slowed down by hardware, so my rig is pretty beefy. It has a GTX 1070 graphics card, which is capable of training large neural networks. I have an intel 8600K CPU, which is a 6 core processor; it's great for multitasking. Couple that with a fast solid state drive, and I've got a pretty fast system."
    },
    "addingsoftware": {
      "title": "ScoutTech Software",
      "dateStart": "April 2017",
      "dataEnd": "June 2017",
      "blurb": "Software created for ScoutTech Outfitters to automate the adding of products to various databases.",
      "img": "0.jpg",
      "links": [
        "https://github.com/SinclairHudson/ScoutTech-Adding-Software"
      ],
      "tags": [
        "Java",
        "Automation"
      ],
      "video": null,
      "url": "/project/addingsoftware",
      "intro": "In the spring of 2017, I was getting pretty good at Java in preparation for the AP exam, and I was looking to apply what I had been studying. At the same time, I was given the boring task of adding products to our database at ScoutTech. The database was slow, mistakes were frequent, a ton of steps were repeated, and it was mind-numbing. So, after doing a bunch of research on the project, I convinced my boss to pay me to develop a desktop that could add products quickly. Over a year later, that same system is being used to add products over twice as fast as the traditional method.",
      "learned": [
        "Agile Development, working closely with the client",
        "Writing and reading large amounts of data in CSV files in Java",
        "Object Oriented Programming experience in the real world",
        "Java Swing UI"
      ],
      "gallery": [
        {
          "src": "1.jpg",
          "width": "699",
          "height": "710"
        },
        {
          "src": "2.jpg",
          "width": "540",
          "height": "960"
        },
        {
          "src": "3.jpg",
          "width": "1413",
          "height": "868"
        }
      ],
      "custom": [],
      "howItWorks": "The design is pretty simple. ChannelAdvisor, the database, accepts two forms of products: Standards and Bundles. Standards are for stand-alone products, like tents, stoves, shovels, etc. Bundles are for very similar products, like apparel that is the same except for size and colour. In each bundle, there is a parent and multiple children. The child products inherit most of their information from the parent. After the user of the Java Program chooses Bundle or Standard, They are greeted with the information page. There are around 12 fields that need to be filled out to make a valid product. When they are filled out, the program stores every product as an object in the system memory. The user can continue to create products and they will be saved in the same way. Once the user clicks “export” on the main page, the program takes all the products in its memory and writes them to a CSV file according to the Channeladvisor formatting rules. The resulting CSV file can then be uploaded directly into ChannelAdvisor."
    }
  },
  "Experience": {
    "huawei": {
      "title": "LiDAR Perception Researcher at Huawei",
      "dateStart": "Jan 2020",
      "dataEnd": "April 2020",
      "blurb": "For my second coop, I worked at a self-driving lab for Huawei.",
      "img": "0.jpg",
      "tags": [
        "Computer Vision",
        "PyTorch",
        "Research",
        "OpenCV",
        "LiDAR Segmentation"
      ],
      "gallery": [
        {
          "src": "0.jpg",
          "width": "320",
          "height": "320"
        },
        {
          "src": "3d.gif",
          "width": "800",
          "height": "800"
        },
        {
          "src": "confmatrix_30.png",
          "width": "640",
          "height": "480"
        },
        {
          "src": "Figure_1.png",
          "width": "3253",
          "height": "792"
        },
        {
          "src": "Figure_2.png",
          "width": "3775",
          "height": "800"
        }
      ],
      "video": null,
      "url": "/experience/huawei",
      "intro": "For four months in 2020, I worked at the Huawei autonomous vehicle lab in Markham. It was an amazing experience; I got to contribute to deep learning research, specifically in LiDAR segmentation. I kept up with current research, built and ran experiments, and even wrote a summary report.",
      "learned": [
        "Implementing models from scratch, based on research papers.",
        "The planning, execution, and documentation of dozens of deep learning experiments",
        "Analysing a dataset (SemanticKITTI) to create a good train/test/val split, and inform different experiments",
        "Reading a lot of research and summarizing it into useful contributions and interesting ideas."
      ],
      "custom": [
        {
          "header": "Summary Paper",
          "body": "Deep learning moves really fast. I was working on the SemanticKITTI dataset, which was only released in late 2019. Despite this, the competition gets weekly submissions, and more than a dozen papers have used it to evaluate their ideas. At Huawei, I was tasked with reading all these relevant papers, and condensing the main ideas into a single summary paper. This paper would be a great resource for anyone who wanted to enter the competition, and it would also be a nice reference for the team. We would be able to easily peruse the useful ideas of over a dozen papers, and incorporate them into our own experiments."
        }
      ]
    },
    "watonomous": {
      "title": "Watonomous Design Team",
      "dateStart": "April 2019",
      "dataEnd": "Present",
      "blurb": "Watonomous is the Self-Driving Car team at the University of Waterloo. We compete in the SAE autodrive challenge, striving to create an autonomous vehicle.",
      "img": "0.jpg",
      "tags": [
        "Computer Vision",
        "TensorFlow",
        "OpenCV",
        "YOLO",
        "Keras"
      ],
      "url": "/experience/watonomous",
      "intro": "I am currently on the Perception team, which is the team's front line. We take in the data from cameras, LIDAR, and radar and attempt to process that data to pull out meaningful features.",
      "learned": [
        "OpenCV masking, searching",
        "Training YOLOv3 on a traffic light dataset",
        "Collaboration and Version Control on a huge, highly integrated codebase",
        "ROS",
        "OpenVino",
        "Managing a small group of developers to complete a larger goal",
        "Code Reviewing",
        "Two-week build, test, deploy cycles with a large team and system"
      ],
      "gallery": [
        {
          "src": "1.jpg",
          "width": "3456",
          "height": "4608"
        },
        {
          "src": "3.jpg",
          "width": "3456",
          "height": "4608"
        },
        {
          "src": "2.jpg",
          "width": "3456",
          "height": "4608"
        },
        {
          "src": "4.jpg",
          "width": "3456",
          "height": "4608"
        }
      ],
      "video": null,
      "description": "I've worked on a few systems within perception. I experimented in railroad bar detection, and I've trained a YOLOv3 model on Traffic Light detection.",
      "custom": [
        {
          "header": "Technical Lead",
          "body": "My current position in the team is that of a Technical Lead on the Perception Sub-team. I'm assigned a project, such as cyclist detection, road line detection, etc. I then lead a small group of core members to accomplish said task. This usually involves processing data, training a neural network, and evaluating it. I'm also responsible for keeping up to date on research in the machine vision space, and understanding how the whole watonomous codebase works. This means that I do a lot of code reviews, pull requests, and debugging."
        }
      ]
    },
    "truenorth": {
      "title": "True North 2019",
      "dateStart": "June 2019",
      "dataEnd": "June 2019",
      "blurb": "Technology for good conference in Kitchener, Ontario.",
      "img": "0.jpg",
      "tags": [
        "Ethics in Tech"
      ],
      "url": "/experience/truenorth",
      "intro": "I attended The True North Tech Conference with my friends from the Wawanesa Insurance innovation outpost. The conference was hosted by Communitech, in Kitchener. There conference focused around tech for good. Privacy, security, implications of new technology. ",
      "learned": [
        "Ethics in technology",
        "Providing digital solutions to social issues"
      ],
      "video": null,
      "gallery": [
        {
          "src": "1.jpg",
          "width": "3456",
          "height": "4608"
        },
        {
          "src": "3.jpg",
          "width": "3456",
          "height": "4608"
        },
        {
          "src": "2.jpg",
          "width": "3456",
          "height": "4608"
        }
      ]
    },
    "wawanesa": {
      "title": "Wawanesa Insurance Developer",
      "date-start": "April 2019",
      "date-end": "August 2019",
      "blurb": "For my first co-op term, I was working in Communitech for the Wawanesa Insurance Innovation Lab.",
      "intro": "I couldn't have asked for a better first coop. My job was to build proof-of-concept projects for Wawanesa Insurance, exploring potential applications of new technologies to the insurance industry. I completed three major projects, and was exposed to so many different frameworks and technologies. I learned so much, and learned while building.",
      "img": "1.jpg",
      "tags": [
        "AWS",
        "React",
        "Node.js",
        "React Native",
        "Frontend",
        "NLP",
        "BERT and Transformers",
        "Jupyter Notebooks"
      ],
      "learned": [
        "EC2, Sagemaker, Comprehend, Lambda, DynamoDB, API Gateway, ECR, Serverless",
        "React",
        "React Native",
        "Frontend",
        "NLP",
        "BERT and Transformers",
        "Jupyter Notebooks"
      ],
      "custom": [
        {
          "header": "Twitter Sentiment Analysis",
          "body": "My first proof of concept project was to create a system that helped a public relations team informed on the public’s opinions. Using Twitter’s API, I created a system that took tweets with a specific keyword and performed sentiment analysis on them. By storing the data in a database, the system is able to track Twitter user’s opinions on the keyword. The whole project was built in AWS with Serverless. I used AWS Lambda, DynamoDB, Comprehend, and API Gateway. My Node.js code tied all those systems together."
        },
        {
          "header": "Instant Verification",
          "body": "I built a similar system for Instagram, but for a different purpose. A big issue in the insurance industry is fraud. Thankfully, quite a few people incriminate themselves on social media, by posting things that prove their claim is false. I designed a system that looks at a user’s instagram profile for posts relating to the claim, whether it be travel, automotive, etc. This data is retrieved with a flask application, and then sent off to a React frontend to be reviewed."
        }
      ],
      "video": null,
      "gallery": [
        {
          "src": "1.jpg",
          "width": "1",
          "height": "1"
        },
        {
          "src": "3.jpg",
          "width": "3456",
          "height": "4608"
        },
        {
          "src": "2.jpg",
          "width": "4608",
          "height": "3456"
        },
        {
          "src": "4.jpg",
          "width": "2457",
          "height": "4603"
        },
        {
          "src": "5.jpg",
          "width": "3456",
          "height": "4608"
        },
        {
          "src": "6.jpg",
          "width": "3456",
          "height": "4608"
        }
      ],
      "url": "/experience/wawanesa"
    },
    "camping": {
      "title": "Camping Trips",
      "dateStart": "Super Early",
      "dataEnd": "Present",
      "blurb": "One of my more intense hobbies is camping, specifically canoe tripping.",
      "img": "0.jpg",
      "tags": [
        "Planning",
        "Management",
        "Communication",
        "White Water Canoeing"
      ],
      "url": "/experience/camping",
      "intro": "Ever since I was little, I was going on camping trips with my family; my father is an outdoor enthusiast. My family goes on a canoe trip just about every summer, and recently we’ve been doing whitewater paddling down various rivers in Northern Ontario. Through the Scouts Canada program, I’ve also been exposed to lightweight backpacking trips and winter camping.",
      "learned": [
        "Quick Decision making and communication skills",
        "Logistics and planning",
        "Teamwork and perseverance"
      ],
      "gallery": [
        {
          "src": "1.jpg",
          "width": "5312",
          "height": "2988"
        },
        {
          "src": "3.jpg",
          "width": "2736",
          "height": "3648"
        },
        {
          "src": "2.jpg",
          "width": "2448",
          "height": "2448"
        },
        {
          "src": "4.jpg",
          "width": "540",
          "height": "960"
        }
      ],
      "custom": [
        {
          "header": "Most Recent Trip",
          "body": "My most recent trip was a 12-Day canoe trip down the Missinaibi River. We went from Dog Lake to Mattice, with a lot of rain and whitewater in between. Below is a video I made from the GoPro footage and pictures taken on the trip. I edited it all in Adobe Premiere Pro, and there's a longer version for those interested in all the whitewater."
        }
      ],
      "video": "https://www.youtube.com/embed/jbu0bRDEHXU"
    },
    "shad": {
      "title": "SHAD UNB 2017",
      "dateStart": "July 2017",
      "dataEnd": "July 2017",
      "blurb": "A month long STEAM and Entrepreneurship program, at the University of New Brunswick.",
      "img": "cave.jpg",
      "tags": [
        "Entrepreneurship",
        "Leadership",
        "Networking"
      ],
      "url": "/experience/shad",
      "intro": "In the July of 2017, I attended the SHAD Program at The University of New Brunswick. SHAD is an educational summer program for exceptional youth across Canada, and it focuses on entrepreneurship. It was a month packed with fun and interesting experiences. We did robotics, went camping, and organized events. I also worked in a team of eight to create a mock business, which we pitched to ‘investors’ in a Shark Tank fashion.",
      "learned": [
        "Rapidly networking with 79 very interesting people.",
        "The basics of entrepreneurship in a team of 8.",
        "Teamwork and Leadership."
      ],
      "gallery": [
        {
          "src": "hill.jpg",
          "width": "960",
          "height": "720"
        },
        {
          "src": "cave.jpg",
          "width": "1",
          "height": "1"
        },
        {
          "src": "hopewell.jpg",
          "width": "960",
          "height": "774"
        },
        {
          "src": "pier.jpg",
          "width": "750",
          "height": "526"
        },
        {
          "src": "uni.jpg",
          "width": "2048",
          "height": "1152"
        }
      ],
      "video": null
    }
  }
}