{"Project": {"ajax": {"name": "ajax", "priority": 40, "title": "AJAX Speech Recognition", "dateStart": "Sept 2018", "dataEnd": "Feb 2019", "blurb": "Voice Recognition system built with Google Tensorflow.", "img": "0.jpg", "links": ["https://github.com/SinclairHudson/AJAX"], "tags": ["TensorFlow", "NLP", "NumPy", "Tensorboard", "Keras", "LSTM"], "gallery": [{"src": "0.jpg", "width": 320, "height": 320}, {"src": "1.jpg", "width": 1527, "height": 472}, {"src": "2.jpg", "width": 3264, "height": 4352}, {"src": "3.jpg", "width": 361, "height": 286}], "video": null, "url": "/project/ajax", "intro": "This is my oldest TensorFlow project, which all started with a frustration with a horrible voice recognition library. I decided to give it a shot myself, using Google TensorFlow. Currently, I have some parts working with a sub-optimal accuracy, and I'm looking into different models and methods to achieve great voice recognition. You can find all my code on my GitHub.\n", "learned": ["Implementing and testing sequence classification neural networks", "Google Tensorflow, NumPy, Pyaudio, and many other small libraries", "Patience", "A Research, Build, Test cycle to complete a project"], "custom": [], "howItWorks": "Currently, I've only trained a word classifier working, capable at differentiating 18 different words apart, with about 92% accuracy. The idea was to determine where the word breaks were, partition the sound file into words, and then classify from there.\n"}, "balltracking": {"name": "balltracking", "priority": 200, "title": "Ball Tracking", "dateStart": "May 2023", "dataEnd": "Dec 2023", "blurb": "An object tracking pipeline used to add cool effects to sports balls in videos", "img": "debug.png", "links": ["https://github.com/SinclairHudson/ball-tracking"], "tags": ["Object Tracking", "OpenCV", "Python", "Kalman Filters", "Object Detection"], "gallery": [{"src": "debug.png", "width": 320, "height": 320}, {"src": "contrail.png", "width": 320, "height": 320}, {"src": "fully_connected_neon.png", "width": 320, "height": 320}, {"src": "neon_line.png", "width": 300, "height": 300}], "video": null, "url": "/project/balltracking", "intro": "This project is the descendant of a hackathon project that I worked on in 2021 (JTrack). I wanted to create an application, mostly for juggling, that could add cool effects to balls moving around in a video. I took my time to architect and build what's effectively  an autonomous vehicle perception stack. There are a lot of effects available now, and it uses a simple Python CLI.\n", "learned": ["pytest", "GitHub Actions", "Gradio GUI", "Applying a Kalman Filter, and a general object detection stack", "Building a nice CLI"], "custom": [{"header": "What's next?", "body": "I'd like to get the detections to work better; currently the video has to be perfect, with well-exposed tennis balls. I'm just using pre-trained NNs right now for detection, so I should probably train one specifically for sports balls."}], "howItWorks": "The pipeline works in 3 stages: detection, tracking, and effects. In between these stages, the state is stored in numpy files. This is so that if something goes wrong, detections (which are expensive to produce) are able to be reused. The detection simply goes through all the frames and detects balls. The tracking stage takes those detections and creates tracks out of them, with a fairly simple hungarian matching algorithm between tracks and detections. The birth and death conditions are also fairly basic (for now). After that, the only thing left to do is to draw the tracks. Those are accomplished using the many features of OpenCV.\n"}, "cansofcom": {"name": "cansofcom", "priority": 100, "title": "CANSOFCOM: RADAR drone Classification", "dateStart": "Jan 2021", "dataEnd": "Jan 2021", "blurb": "Drone Classifiction based on RADAR return signal, Hack the North 2020++.", "img": "0.png", "links": ["https://github.com/SinclairHudson/CANSOFCOM"], "tags": ["Signal Processing", "PyTorch", "Classification"], "gallery": [{"src": "0.png", "width": 320, "height": 320}, {"src": "1.png", "width": 1049, "height": 888}, {"src": "2.png", "width": 850, "height": 702}, {"src": "3.png", "width": 394, "height": 278}, {"src": "4.png", "width": 437, "height": 340}], "video": null, "url": "/project/cansofcom", "intro": "I coded this project up in 48 hours for a challenge put on by the Canadian  Special Operations Forces Command (CANSOFCOM). The challenge was to classify commercial drones based on the noisy return signal from a RADAR. I highly recommend you take a look at the Jupyter notebook that I've posted on Github; I wrote it up to present my results in the competition, and it gives an  executive summary of the project.\n", "learned": ["Fourier transforms and general signal processing terminology and techniques.", "Building a research-type repository super-fast.", "Signal-To-Noise ratios, Long window and short window STFTs."], "custom": [{"header": "Let's talk about noise", "body": "The data for this project was simulated at 3 different noise levels and with two different sampling frequencies. My neural network performed exceptionally on everything but 0 dB SNR (Signal to Noise Ratio). I plan on expanding the project to handle more noise, lower sampling frequencies, and more challenges. This was a first pass for the hackathon; there's still a lot to be done."}, {"header": "Results", "body": "I ended up winning first place in this competition for Hack the North 2020++. It's the first time I've come first place in a hackathon competition :)."}], "howItWorks": "There are actually a lot of interesting features to this problem. As the rotors of a drone spin around, the radio waves from the RADAR hit the blades, and their wavelength shortens (or lengthens), because of the doppler effect. So what you get in the return signal is periodic bursts of higher frequencies. These bursts can be seen in the short-window fourier transforms, and the icon for this project depicts one of those bursts. These bursts look different  depending on the size of the rotor blades, how fast they're spinning, and other drone parameters. I decided to feed the short-window fourier transform of the signal to a convolutional neural network, to classify which drone the signal originated from.\n"}, "jtrack": {"name": "jtrack", "priority": 50, "title": "JTrack: Juggling Prop Tracking Software", "dateStart": "Jan 2021", "dataEnd": "Jan 2021", "blurb": "Silly little project that allows you to track the position and velocity of juggling balls.", "img": "0.jpg", "links": ["https://github.com/SinclairHudson/JTrack", "https://devpost.com/software/jtrack-u0oksh"], "tags": ["Object Tracking", "OpenCV", "Python", "Kalman Filters"], "gallery": [{"src": "0.jpg", "width": 320, "height": 320}, {"src": "comet.jpg", "width": 320, "height": 320}, {"src": "consistency.jpg", "width": 522, "height": 460}, {"src": "rainbow.jpg", "width": 524, "height": 422}], "video": "https://www.youtube.com/embed/jWXFtPx-wpo", "url": "/project/jtrack", "intro": "I coded this project up in 36 hours for Hack The North 2020++. It tracks juggling balls in a video, so that you can add informative and creative effects to them. There are some pretty cool and informative results.\n", "learned": ["Colour filtering and simple blob detection in OpenCV", "Applying a Kalman Filter, and a general object detection stack"], "custom": [{"header": "What's next?", "body": "I want to make this software available to the general juggling community, so that we can experiment, analyse, teach, and show off. So I need to round out the user interface, make it very consistent, and add a lot of effects. Right now detections are the weakest link; it works well for clementines, but not much else."}], "howItWorks": "In object tracking, we call an object a track. There's a standard loop, that executes every frame. We first advance the Kalman filters for every track, which have kinematics equations to predict parabolic motion. Then, we use OpenCV to detect the locations of the balls. We then use the hungarian matching algorithm to find the best  paring between detections and predictions (tracks). In the complete bipartite graph connecting predictions  to detections, the cost is Euclidean distance in the image coordinate space. Once we've done association, we update the Kalman filters for every track, and we add new tracks  to detections that weren't paired up. We also delete old tracks; ones that we haven't seen in a while. This goes on for every frame, and of course once we have this information we're free to do anything with it. I like plotting apexes of throws, velocity-dependent colour trails, etc.  The states of the Kalman filters contain estimates of position, velocity, and even acceleration.\n"}, "vm": {"name": "vm", "priority": 80, "blurb": "For the final project of CS246E, a partner and I built a version of VIM in C++.", "custom": [], "date-end": "Dec 2019", "date-start": "Nov 2019", "gallery": [{"height": "5", "src": "0.png", "width": "5"}, {"height": "496", "src": "VM.png", "width": "734"}], "howItWorks": "In the end, our project centered around the Command design pattern. Essentially, all commands were represented as Command objects. There was a command parser class, which was in charge of converting keyboard input into Command objects. Keep in mind that commands in VIM can be repeated, like \u20186fc\u2019. The commands were stored on a stack structure to keep undos clean. We had a class that handled the text being manipulated, and file management. We also had a class for displaying the content, which followed the Observer design pattern to update when the contents or cursor changed.", "img": "0.png", "intro": "For the final project of CS246E, we were instructed to implement vim in C++. It was a massive object-oriented design project. In total, we had to support 56 commands, including macros and unbounded undos. My partner and I spent weeks planning out our design, making sure every command could be implemented easily. We used many design patterns and SOLID principles to create an excellent design that made implementation straightforward. In the end our complete coverage of the specification, as well as a few bonus features, earned us a grade of 104%.", "learned": ["Applying SOLID Principles", "Applying Model-View-Controller (MVC) architecture", "Writing a very detailed design report, along with a formal UML", "Pair programming with a partner", "Applying Object-Oriented design patterns", "Project planning and software design"], "links": [], "tags": ["C++", "Systems Design", "Design Patterns"], "title": "VM", "url": "/project/vm", "video": null}, "synviz": {"name": "synviz", "priority": 50, "blurb": "Placing 3rd in UofTHacks VII, Synviz is a wearable device that reads lips.", "custom": [{"body": "There are a lot of things that could be added to this project fairly easily. I could mess around with the hyperparameters a little bit more to improve accuracy, I could photograph and tag more data, and I could add more gestures, like one for skip track. Top priority would be to increase the size of the dataset. I created and tagged the dataset myself, in one afternoon, in the same room, in the same clothes. Needless to say, the system isn\u2019t really robust, since it\u2019s only trained on 1500 similar frames. Sigh\u2026 tagging computer vision data is such a bore, but that\u2019s the only surefire way to improve my model.", "header": "What's Next?"}], "dataEnd": "Jan 2020", "dateStart": "Jan 2020", "gallery": [{"height": "320", "src": "prototype.jpg", "width": "320"}, {"height": "973", "src": "pipeline.jpg", "width": "1347"}, {"height": "497", "src": "team_photo.jpg", "width": "1514"}], "github": "https://github.com/SinclairHudson/Synviz", "howItWorks": "The system is pretty easy to understand on a high level. There\u2019s an infinite loop, taking images from the webcam that\u2019s attached to my wall, and feeding them to a neural network. The YOLOv3 network is nice and fast, with enough accuracy to pick up my hands and classify them. The network not only detects where the hands are, but classifies them as one of 6 hand states: palm, back, flat, vertical, fist, and neutral. After the network has given its predictions, it\u2019s just a matter of simple logic to perform the audio modifications through terminal commands.", "img": "prototype.jpg", "intro": "My friend Martin came across a cool paper, \"Deep Lip Reading: a comparison of models and an online application\" by Afouras et al. In the paper, the authors try multiple pipelines, trying to ", "learned": ["Creating a deep learning pipeline, deploying a custom model.", "Building a very compact prototype.", "Pitching well at a large hackathon"], "links": ["https://devpost.com/software/synviz", "https://github.com/SinclairHudson/Synviz"], "tags": ["TensorFlow", "GCP", "Raspberry Pi", "OpenCV"], "title": "Synviz", "url": "/project/synviz", "video": null, "wandb": null}, "tripwire": {"name": "tripwire", "priority": 12, "blurb": "An app built in React Native that triggers alarms when in proximity to GPS coordinates.", "custom": [], "dataEnd": "June 2019", "dateStart": "June 2019", "gallery": [{"height": "2340", "src": "1.jpg", "width": "1080"}, {"height": "2340", "src": "3.jpg", "width": "1080"}, {"height": "2340", "src": "2.jpg", "width": "1080"}, {"height": "320", "src": "0.jpg", "width": "320"}, {"height": "4608", "src": "4.jpg", "width": "3456"}, {"height": "3456", "src": "5.jpg", "width": "4608"}], "howItWorks": "The app uses the GPS in the phone for its main functionality. The user can set certain coordinates as waypoints, and specify what kind of alarm should trigger when the phone came within a specified radius of the point.", "img": "0.jpg", "intro": "This was a project I created with three of my friends for EngHack 2019. We decided to tackle a light problem: we were all missing our public transit stops because we were reading, sleeping, or on our phones. Obviously, we needed a way to alert us when we got close to a bus stop or specific location. We were all new to React Native, so it was pretty rough, but we all had a good time.", "learned": ["React Native, specifically navigation in React Native", "Advanced Git VC to get four devs working together on one project", "Rapid Prototyping in a team"], "links": ["https://github.com/SinclairHudson/tripwire", "https://devpost.com/software/tripwire"], "tags": ["React Native", "EngHack 2019", "UI/UX"], "title": "Tripwire", "url": "/project/tripwire", "video": null}, "addingsoftware": {"name": "addingsoftware", "priority": 1, "blurb": "Software created for ScoutTech Outfitters to automate the adding of products to various databases.", "custom": [], "dataEnd": "June 2017", "dateStart": "April 2017", "gallery": [{"height": "710", "src": "1.jpg", "width": "699"}, {"height": "960", "src": "2.jpg", "width": "540"}, {"height": "868", "src": "3.jpg", "width": "1413"}], "howItWorks": "The design is pretty simple. ChannelAdvisor, the database, accepts two forms of products: Standards and Bundles. Standards are for stand-alone products, like tents, stoves, shovels, etc. Bundles are for very similar products, like apparel that is the same except for size and colour. In each bundle, there is a parent and multiple children. The child products inherit most of their information from the parent. After the user of the Java Program chooses Bundle or Standard, They are greeted with the information page. There are around 12 fields that need to be filled out to make a valid product. When they are filled out, the program stores every product as an object in the system memory. The user can continue to create products and they will be saved in the same way. Once the user clicks \u201cexport\u201d on the main page, the program takes all the products in its memory and writes them to a CSV file according to the Channeladvisor formatting rules. The resulting CSV file can then be uploaded directly into ChannelAdvisor.", "img": "0.jpg", "intro": "In the spring of 2017, I was getting pretty good at Java in preparation for the AP exam, and I was looking to apply what I had been studying. At the same time, I was given the boring task of adding products to our database at ScoutTech. The database was slow, mistakes were frequent, a ton of steps were repeated, and it was mind-numbing. So, after doing a bunch of research on the project, I convinced my boss to pay me to develop a desktop that could add products quickly. Over a year later, that same system is being used to add products over twice as fast as the traditional method.", "learned": ["Agile Development, working closely with the client", "Writing and reading large amounts of data in CSV files in Java", "Object Oriented Programming experience in the real world", "Java Swing UI"], "links": ["https://github.com/SinclairHudson/ScoutTech-Adding-Software"], "tags": ["Java", "Automation"], "title": "ScoutTech Software", "url": "/project/addingsoftware", "video": null}, "conduct": {"name": "conduct", "priority": 70, "blurb": "This project allows me to control audio playback on my computer by making gestures with my hands. It uses a YOLOv3 algorithm on webcam input.", "custom": [{"body": "There are a lot of things that could be added to this project fairly easily. I could mess around with the hyperparameters a little bit more to improve accuracy, I could photograph and tag more data, and I could add more gestures, like one for skip track. Top priority would be to increase the size of the dataset. I created and tagged the dataset myself, in one afternoon, in the same room, in the same clothes. Needless to say, the system isn\u2019t really robust, since it\u2019s only trained on 1500 similar frames. Sigh\u2026 tagging computer vision data is such a bore, but that\u2019s the only surefire way to improve my model.", "header": "What's Next?"}], "dataEnd": "August 2019", "dateStart": "June 2019", "gallery": [{"height": "320", "src": "0.jpg", "width": "320"}, {"height": "973", "src": "1.jpg", "width": "1347"}, {"height": "497", "src": "2.jpg", "width": "1514"}], "howItWorks": "The system is pretty easy to understand on a high level. There\u2019s an infinite loop, taking images from the webcam that\u2019s attached to my wall, and feeding them to a neural network. The YOLOv3 network is nice and fast, with enough accuracy to pick up my hands and classify them. The network not only detects where the hands are, but classifies them as one of 6 hand states: palm, back, flat, vertical, fist, and neutral. After the network has given its predictions, it\u2019s just a matter of simple logic to perform the audio modifications through terminal commands.", "img": "0.jpg", "intro": "I\u2019m always listening to music while I work, and I thought it would be cool to control my music through very intuitive hand gestures. This is one case where voice recognition gets unreliable; if you\u2019re blasting music, the microphone has trouble picking up your commands. So, you use your hands to control the computer, like a conductor directs a symphony.", "learned": ["YOLOv3", "Tuning hyperparameters", "Learning rate decay and warmup epochs to help a network converge", "Processing and displaying outputs using OpenCV"], "links": [], "tags": ["TensorFlow 2.0", "Computer Vision", "YOLOv3", "OpenLabeling", "OpenCV"], "title": "Conduct", "url": "/project/conduct", "video": "https://www.youtube.com/embed/CPZNZpvM5Xc"}, "speaker": {"name": "speaker", "priority": 2, "blurb": "I built a speaker and amplifier circuit from scratch using components found at a hardware store, soldering it all together and testing it with an oscilliscope", "custom": [], "date-end": "July 2018", "date-start": "June 2018", "gallery": [{"height": "5", "src": "0.jpg", "width": "5"}, {"height": "4608", "src": "1.jpg", "width": "3456"}, {"height": "747", "src": "2.jpg", "width": "1286"}, {"height": "2444", "src": "3.jpg", "width": "3300"}, {"height": "3264", "src": "4.jpg", "width": "3264"}, {"height": "824", "src": "5.jpg", "width": "1013"}, {"height": "4352", "src": "6.jpg", "width": "3264"}], "howItWorks": "Currently, I've only trained a word classifier working, capable at differentiating 18 different words apart, with about 92% accuracy. The idea was to determine where the word breaks were, partition the sound file into words, and then classify from there. However, with a low classification accuracy, I'm currently looking into a new approach, using a sequence to sequence LSTM network to turn the sound byte into phenomes first. The good news is that I already have the data; I wrote a python script that automatically labels and records my voice, making data collection easy.", "img": "0.jpg", "intro": "During my Grade 12 Year, I had the wonderful opportunity to work with an ex-Hydro One electrical engineer, James Whatley, in my Computer Engineering 12 Class. We learned and then built voltage regulation circuits, amperage regulations circuits, full-wave rectifiers, but we were short on time at the end of the year. Eager to learn more, I talked a bit with my instructor and we agreed to meet for a few days in the early summer to work on another project: the portable speaker.", "learned": ["Operational Amplifier circuits", "Using an oscilliscope to debug physical circuits", "Audio signal processing, filtering, and amplifying", "Current and Voltage limiting circuits", "Soldering complex circuits from schematics"], "tags": ["Hardware", "Soldering", "3D Modelling", "3D Printing", "Oscilliscope"], "title": "DIY Speaker", "url": "/project/speaker", "video": "https://www.youtube.com/embed/BCQs8cIpOGc"}, "waxmuseum": {"name": "waxmuseum", "priority": 3, "url": "/project/waxmuseum", "title": "Wax Museum", "dataEnd": "Dec 2019", "dateStart": "Sept 2019", "blurb": "This project uses laptop footage to turn off the laptop screen when I'm not looking at it.", "img": "0.png", "intro": "Two problems. I don't know PyTorch, and my laptop battery keeps dying during lecture. The latter has easier solutions. I could lower my brightness, reduce non-essential usage, or remember to charge my laptop. However, I'm a bit of an AI geek and I had an unreasonable yet simple solution in mind: turn off the screen when I'm not looking at it, and turn it back on when I am. I just need a simple convolutional net to perform binary classification. This will stretch my battery life further, and give me a chance to learn PyTorch. Problems solved.", "custom": [{"header": "Weights and Biases", "body": "Weights and Biases is a new analytics platform for training neural networks. It's like TensorBoard, but better in functionality and easier to use. It allows you to compare runs effortlessly, sweep through hyperparameters, and display custom metrics. I used Weights and Biases in this project to find optimal hyperparameters like the learning rate, and to display performance metrics so I could pick the best model. I also displayed a bunch of activation maps, because they're really cool. :)"}], "gallery": [{"height": "480", "src": "0.png", "width": "480"}, {"height": "480", "src": "1.png", "width": "640"}], "howItWorks": "This network is a simple convolutional one, with about 4 layers. I use OpenCV to get frames from the webcam, and then run them through the network. I then use the past 3 readings to smooth the output. If the network says I'm looking away, then I dim the screen through the command line. Otherwise, I keep the screen lit. It's only slightly more complex than an if statement in an infinite loop.", "learned": ["Weights & Biases analytics", "Tuning hyperparameters", "PyTorch"], "links": ["https://github.com/SinclairHudson/wax-museum", "https://app.wandb.ai/sinclairhudson/waxmuseum?workspace=user-sinclairhudson"], "tags": ["Computer Vision", "PyTorch", "Weights & Biases", "OpenCV"], "video": null}, "sim2real": {"name": "sim2real", "priority": 150, "blurb": "While working at WATonomous, my team and I explored lane detection using simulation data and unlabeled real data, performing domain adaptation from the simulation to real-life.", "custom": [{"header": "IV 2022", "body": "As of writing, the paper was accepted into the Intelligent Vehicles 2022 Symposium, in Aachen, Germany. I'm likely going to travel there in June 2022 to present our work."}], "dataEnd": "Jun 2022", "dateStart": "Jun 2021", "gallery": [{"src": "lanes.png", "height": "320", "width": "320"}, {"src": "model.png", "height": "320", "width": "320"}], "github": "https://github.com/anita-hu/sim2real-lane-detection", "howItWorks": "We tested multiple methods. Our highest performing methods featured networks that mapped both the real images and simulation images into a shared latent space. Then, a detector based on Ultra-Fast Lane Detection would predict keypoints in the image from that latent space. Our experiments looked at the various ways we could enforce the latent space distribution from simulated data and real data to be similar. We tried Adversarial Discriminative techniques with a discriminator network, and other forms of cyclic consistency losses commonly used in state-of-the-art unpaired image translation models.", "img": "lanes.png", "intro": "Real-labelled data is expensive for AV. There is also a lack of open-source datasets for lane detection. So my group at WATonomous set out on a research project to create a good lane detection model using only simulation data and unlabelled real-world data. This is usually called sim-to-real domain adaptation, and it's of very high importance in the AV industry, to mitigate the cost of finding high-quality data.", "learned": ["Submitting to a conference, making continual revisions on a paper", "presenting work at a conference", "Running meaningful experiments in a research repository that brings together multiple ideas"], "links": ["https://arxiv.org/abs/2202.07133", "https://github.com/anita-hu/sim2real-lane-detection"], "tags": ["IV 2022"], "title": "Sim-to-Real domain Adaptation for Lane Detection", "url": "/project/sim2real", "video": null, "wandb": null}, "custompc": {"name": "custompc", "priority": 8, "blurb": "I built and now maintain my own PC. You could say I'm a PC enthusiast.", "custom": [], "dataEnd": "Jan 2018", "dateStart": "June 2016", "gallery": [{"height": "4099", "src": "1.jpg", "width": "3377"}, {"height": "960", "src": "3.jpg", "width": "540"}, {"height": "4245", "src": "2.jpg", "width": "3455"}, {"height": "4063", "src": "0.jpg", "width": "3456"}], "howItWorks": "I don't like being slowed down by hardware, so my rig is pretty beefy. It has a GTX 1070 graphics card, which is capable of training large neural networks. I have an intel 8600K CPU, which is a 6 core processor; it's great for multitasking. Couple that with a fast solid state drive, and I've got a pretty fast system.", "img": "4.jpg", "intro": "Over the last few years, I have slowly built and upgraded my personal computer. The project started in grade 10 when I dissected a few old computers from my father\u2019s workplace. From there, I slowly purchased and salvaged parts for the the next two years, arriving at this system. Along the way, I learned a great deal about how PCs work, and how to repair them. I have also helped a few friends out with building their own systems. My current build dual-boots Windows and Ubuntu.", "learned": ["How all components of a computer are installed, and how they all work together", "How to design a cost-effective build that has all the features I need", "Configuring BIOS settings, and dual booting", "File system permissions and formatting"], "tags": ["Hardware", "Dual-Booting"], "title": "Custom PC", "url": "/project/custompc", "video": null}, "website": {"name": "website", "priority": 10, "blurb": "This website is built by myself from scratch, designed in Illustrator, and coded using React.", "custom": [], "date-end": "Present", "date-start": "Dec 2018", "gallery": [{"height": "5", "src": "0.jpg", "width": "5"}, {"height": "5432", "src": "1.jpg", "width": "6215"}], "howItWorks": "This website is built with create-react-app, and is highly abstracted. All the data is in one file, and all the styling and structure is in React Components. All the pages are procedurally generated, so if I want to add a project, I just have to put the information into the data file. I used libraries for the image gallery and the Parallax banners, but everything else I coded myself. While a lot of people have told me that I shouldn't re-invent the wheel, I find value in learning how to build these things by myself.", "img": "0.jpg", "intro": "I designed and coded this website entirely by myself in 2018, but it was in pure HTML and CSS. I used Illustrator to lay everything out and get a feel for the design. The project got way to large to maintain, and editing the HTML files for each individual page was a pain. So, I purchased a book on the React Framework, and spun this current website over a few weekends, learning as I went.", "learned": ["React", "Creating dynamic websites, in which things move, slide, and interact", "Project Structuring for large websites"], "links": ["https://github.com/SinclairHudson/sinclairhudson.github.io"], "tags": ["React", "Frontend", "Design", "Illustrator"], "title": "This Website", "url": "/project/website", "video": null}}, "Experience": {"watonomous": {"name": "watonomous", "priority": 60, "title": "Watonomous Design Team", "blurb": "Watonomous is the Self-Driving Car team at the University of Waterloo. We compete in the SAE Autodrive challenge, striving to create an autonomous vehicle.", "custom": [{"body": "My current position in the team is that of a Technical Lead on the Perception Sub-team. I'm assigned a project, such as cyclist detection, road line detection, etc. I then lead a small group of core members to accomplish said task. This usually involves processing data, training a neural network, and evaluating it. I'm also responsible for keeping up to date on research in the machine vision space, and understanding how the whole watonomous codebase works. This means that I do a lot of code reviews, pull requests, and debugging.", "header": "Technical Lead"}], "dataEnd": "Present", "dateStart": "April 2019", "description": "I've worked on a few systems within perception. I experimented in railroad bar detection, and I've trained a YOLOv3 model on Traffic Light detection.", "gallery": [{"height": "4608", "src": "1.jpg", "width": "3456"}, {"height": "4608", "src": "3.jpg", "width": "3456"}, {"height": "4608", "src": "2.jpg", "width": "3456"}, {"height": "4608", "src": "4.jpg", "width": "3456"}], "img": "0.jpg", "intro": "I am currently on the Perception team, which is the team's front line. We take in the data from cameras, LIDAR, and radar and attempt to process that data to pull out meaningful features.", "learned": ["OpenCV masking, searching", "Training YOLOv3 on a traffic light dataset", "Collaboration and Version Control on a huge, highly integrated codebase", "ROS", "OpenVino", "Managing a small group of developers to complete a larger goal", "Code Reviewing", "Two-week build, test, deploy cycles with a large team and system"], "tags": ["Computer Vision", "TensorFlow", "OpenCV", "YOLO", "Keras"], "url": "/experience/watonomous", "video": null}, "darwin": {"name": "darwin", "priority": 80, "blurb": "For my fourth coop, I worked at DarwinAI, on the Customer Experience team.", "intro": "When I arrived at Darwin, the company had just decided it would be pivoting to the manufacturing industry, focusing on quality inspection with AI. I was responsible for organizing proof-of-concept projects for clients, training models and presenting results weekly. I trained some object detectors for Foreign Object Debris (FOD) detection, worked extensively on tabular data, and I also authored two research repositories for the company to use in the future. One of those was an anomaly detection research repository, using autoencoders.", "custom": [{"header": "Anomaly Detection Research", "body": "Although I was on the Customer Experience team, I had the opportunity to build out two research repositories from scratch. One was for anomaly detection in images, using autoencoders. I implemented many autoencoders from current research, and tested their performance on client and public datasets. The main idea there is to train the autoencoder on exclusively \"normal\" images, to achieve a low reconstruction loss on those sorts of images. The autoencoder is not prepared to reconstruct anomalous images, and so a high reconstruction loss correlates with an anomaly. This unsupervised approach to anomaly detection will likely be critical in defect detection cases, since it flags new anomalies as they arise in the manufacturing process; not only the ones in the training set."}, {"header": "Dataset Distillation Research", "body": "One of the things we always struggled with at Darwin was the lack of data. Clients would come to us with hundreds of labelled images when we needed thousands to meet their requirements. For this reason, I started exploring Dataset Distillation in a research repository for the company. The main idea with Dataset Distillation is that not all data samples are created equally. We can actually optimize the data to be super effective at training models. The end goal is to speed up model training exponentially, as we require just a few GD steps to achieve results that would take a model training on normal data thousands of GD steps."}, {"header": "Identity Crisis", "body": "The Customer Experience team I was a part of was in an identity crisis as the company made its pivot. Two internal promotions also changed our team lead. We spent a lot of time as a team talking about our current and future function in a company as we ventured into a new industry. I was able to participate in a lot of discussions that would shape the work of Customer Experience, as we dedicated our time to providing value to the customer."}], "dateStart": "May 2021", "dataEnd": "Aug 2021", "gallery": [{"height": "320", "src": "0.jpg", "width": "320"}, {"src": "2.jpg", "height": "4608", "width": "3456"}], "img": "0.jpg", "learned": ["More about the inner workings of PyTorch", "Authoring research repos", "Presenting technical results to non-technical shareholders"], "tags": ["PyTorch", "Python", "Computer Vision", "Anomaly Detection"], "title": "Deep Learning Developer at DarwinAI", "url": "/experience/darwin", "video": null}, "cruise": {"name": "cruise", "priority": 100, "blurb": "I did my 6th and final coop term working for Cruise, the GM self-driving company.", "intro": "At Cruise, I was on the Model Deployment Platform team (MDP). Specifically, I was responsible for creating Python tooling that we would use to debug models going through deployment, from PyTorch to (usually) TensorRT. In deployment, we'd often get numerical disparity between the optimized model and the original PyTorch model. My tools helped debug these issues, quantifying the errors and making it easy to find the root-cause.", "dateStart": "Sept 2022", "dataEnd": "Dec 2022", "gallery": [{"src": "0.png", "height": "320", "width": "320"}], "img": "0.png", "learned": ["Software development in a well-structured team", "Listening to customer feedback and iterating on my features", "Model deployments, using ONNX and TensorRT"], "tags": ["Autonomous Vehicles", "PyTorch", "ONNX", "Python", "Computer Vision", "Object Detection", "SF"], "title": "Deep Learning Performance Engineer", "url": "/experience/cruise", "video": null}, "shad": {"name": "shad", "priority": 10, "blurb": "A month long STEAM and Entrepreneurship program, at the University of New Brunswick.", "dataEnd": "July 2017", "dateStart": "July 2017", "gallery": [{"height": "720", "src": "hill.jpg", "width": "960"}, {"height": "1", "src": "cave.jpg", "width": "1"}, {"height": "774", "src": "hopewell.jpg", "width": "960"}, {"height": "526", "src": "pier.jpg", "width": "750"}, {"height": "1152", "src": "uni.jpg", "width": "2048"}], "img": "cave.jpg", "intro": "In the July of 2017, I attended the SHAD Program at The University of New Brunswick. SHAD is an educational summer program for exceptional youth across Canada, and it focuses on entrepreneurship. It was a month packed with fun and interesting experiences. We did robotics, went camping, and organized events. I also worked in a team of eight to create a mock business, which we pitched to \u2018investors\u2019 in a Shark Tank fashion.", "learned": ["Rapidly networking with 79 very interesting people.", "The basics of entrepreneurship in a team of 8.", "Teamwork and Leadership."], "tags": ["Entrepreneurship", "Leadership", "Networking"], "title": "SHAD UNB 2017", "url": "/experience/shad", "video": null}, "huawei": {"name": "huawei", "priority": 40, "blurb": "For my second coop, I worked at a self-driving lab for Huawei.", "custom": [{"header": "Summary Paper", "body": "Deep learning moves really fast. I was working on the SemanticKITTI dataset, which was only released in late 2019. Despite this, the competition gets weekly submissions, and more than a dozen papers have used it to evaluate their ideas. At Huawei, I was tasked with reading all these relevant papers, and condensing the main ideas into a single summary paper. This paper would be a great resource for anyone who wanted to enter the competition, and it would also be a nice reference for the team. We would be able to easily peruse the useful ideas of over a dozen papers, and incorporate them into our own experiments."}], "dataEnd": "April 2020", "dateStart": "Jan 2020", "gallery": [{"height": "320", "src": "0.jpg", "width": "320"}, {"height": "800", "src": "3d.gif", "width": "800"}, {"height": "480", "src": "confmatrix_30.png", "width": "640"}, {"height": "792", "src": "Figure_1.png", "width": "3253"}, {"height": "800", "src": "Figure_2.png", "width": "3775"}], "img": "0.jpg", "intro": "For four months in 2020, I worked at the Huawei autonomous vehicle lab in Markham. It was an amazing experience; I got to contribute to deep learning research, specifically in LiDAR segmentation. I kept up with current research, built and ran experiments, and even wrote a summary report.", "learned": ["Implementing models from scratch, based on research papers.", "The planning, execution, and documentation of dozens of deep learning experiments", "Analysing a dataset (SemanticKITTI) to create a good train/test/val split, and inform different experiments", "Reading a lot of research and summarizing it into useful contributions and interesting ideas."], "tags": ["Computer Vision", "PyTorch", "Research", "OpenCV", "LiDAR Segmentation"], "title": "LiDAR Perception Researcher at Huawei", "url": "/experience/huawei", "video": null}, "wawanesa": {"name": "wawanesa", "priority": 30, "blurb": "For my first co-op term, I was working in Communitech for the Wawanesa Insurance Innovation Lab.", "custom": [{"body": "My first proof of concept project was to create a system that helped a public relations team informed on the public\u2019s opinions. Using Twitter\u2019s API, I created a system that took tweets with a specific keyword and performed sentiment analysis on them. By storing the data in a database, the system is able to track Twitter user\u2019s opinions on the keyword. The whole project was built in AWS with Serverless. I used AWS Lambda, DynamoDB, Comprehend, and API Gateway. My Node.js code tied all those systems together.", "header": "Twitter Sentiment Analysis"}, {"body": "I built a similar system for Instagram, but for a different purpose. A big issue in the insurance industry is fraud. Thankfully, quite a few people incriminate themselves on social media, by posting things that prove their claim is false. I designed a system that looks at a user\u2019s instagram profile for posts relating to the claim, whether it be travel, automotive, etc. This data is retrieved with a flask application, and then sent off to a React frontend to be reviewed.", "header": "Instant Verification"}], "date-end": "August 2019", "date-start": "April 2019", "gallery": [{"height": "1", "src": "1.jpg", "width": "1"}, {"height": "4608", "src": "3.jpg", "width": "3456"}, {"height": "3456", "src": "2.jpg", "width": "4608"}, {"height": "4603", "src": "4.jpg", "width": "2457"}, {"height": "4608", "src": "5.jpg", "width": "3456"}, {"height": "4608", "src": "6.jpg", "width": "3456"}], "img": "1.jpg", "intro": "I couldn't have asked for a better first coop. My job was to build proof-of-concept projects for Wawanesa Insurance, exploring potential applications of new technologies to the insurance industry. I completed three major projects, and was exposed to so many different frameworks and technologies. I learned so much, and learned while building.", "learned": ["EC2, Sagemaker, Comprehend, Lambda, DynamoDB, API Gateway, ECR, Serverless", "React", "React Native", "Frontend", "NLP", "BERT and Transformers", "Jupyter Notebooks"], "tags": ["AWS", "React", "Node.js", "React Native", "Frontend", "NLP", "BERT and Transformers", "Jupyter Notebooks"], "title": "Wawanesa Insurance Developer", "url": "/experience/wawanesa", "video": null}, "truenorth": {"name": "truenorth", "priority": 20, "blurb": "Technology for good conference in Kitchener, Ontario.", "dataEnd": "June 2019", "dateStart": "June 2019", "gallery": [{"height": "4608", "src": "1.jpg", "width": "3456"}, {"height": "4608", "src": "3.jpg", "width": "3456"}, {"height": "4608", "src": "2.jpg", "width": "3456"}], "img": "0.jpg", "intro": "I attended The True North Tech Conference with my friends from the Wawanesa Insurance innovation outpost. The conference was hosted by Communitech, in Kitchener. There conference focused around tech for good. Privacy, security, implications of new technology. ", "learned": ["Ethics in technology", "Providing digital solutions to social issues"], "tags": ["Ethics in Tech"], "title": "True North 2019", "url": "/experience/truenorth", "video": null}, "nvidia": {"name": "nvidia", "priority": 90, "blurb": "I spent my fifth coop term working for NVIDIA, doing perception research for autonomous vehicles.", "intro": "I worked on the LiDAR perception sub-team of the autonomous vehicles organization. The internship was still remote, with most of the team on the west coast. I was working on LiDAR object detection, which is quite similar to computer vision but with a couple of very cool quirks and tricks. In my experiments to improve object detection performance, I was doing a lot of 3D point cloud processing, with dense and sparse 3D convolutional networks.", "custom": [{"header": "Pseudo-GT efforts", "body": "A big focus of my coop was improving real-time detection pipelines for use in pseudo-ground truth generation, or auto-labelling. The AV organization has a lot of data coming in, and can't hope to have it all labelled by humans. So my main project was to take a real-time LiDAR object detection network and explore different ideas and methodologies that weren't necessarily real-time or safety certified, to boost detection F-score."}, {"header": "Sparse Tensors", "body": "LiDAR data is very sparse, so one of my big efforts was getting sparse tensors and sparse tensor networks working throughout a whole detection pipeline. For a pseudo-ground truth model, the limiting factor of designing models is the memory, not the runtime. In my case, I was limited to 32GB, which isn't a lot to work with. Sparse tensors allowed me to reduce the activation size of my network by 98%, while improving F-scores. I was using Minkowski Engine, an open source sparse tensor deep learning library from NVIDIA, which has great integration with PyTorch."}], "dateStart": "Jan 2022", "dataEnd": "Apr 2022", "gallery": [{"src": "0.png", "height": "320", "width": "320"}, {"src": "2d.png", "height": "320", "width": "320"}], "img": "0.png", "learned": ["Sparse convolutions and other sparse methods of working with point cloud data", "Evaluating modern point cloud object detection methods on real data", "Working in a highly structured/safety-certified development environment"], "tags": ["Autonomous Vehicles", "PyTorch", "Python", "Computer Vision", "Object Detection", "Auto-labelling", "remote"], "title": "Deep Learning Autonomous Vehicles Researcher", "url": "/experience/nvidia", "video": "https://www.youtube.com/embed/T7w-ZCVVUgM"}, "untether": {"name": "untether", "priority": 70, "blurb": "For my third coop, I worked at Untether.AI. They're a startup that builds specialized cards for accelerated neural network inference.", "intro": "I was on the neural networks team. We were responsible for converting TensorFlow networks into quantization-ready graphs, performing optimizations and quantization during the conversion. The RunAI chip can only perform 8-bit integer calculations, which means the neural networks had to be quantized. There's some very clever math to represent floating point activations, weights, and biases as integer values. Part of my job was converting common neural neural network operations into the integer space.", "custom": [{"header": "Non-Max Suppression", "body": "A good portion of my time at Untether.AI was spent implementing Non-Max Suppression using only integer operations. That's so that NMS can be run on the runAI chip that Untether.AI is developing."}, {"header": "Uncertain Times", "body": "This coop was completely remote, because of the COVID-19 pandemic. As a result, I don't have many pictures."}], "dataEnd": "Dec 2020", "dateStart": "Sept 2020", "gallery": [{"height": "320", "src": "0.jpg", "width": "320"}], "img": "0.jpg", "learned": ["Agile software development, fully integrated with GitHub Enterprise", "Quantization and optimization of neural network operations", "Designing, implementing, and evaluating product features", "Building software while considering the compiler, and the hardware that it will run on."], "tags": ["TensorFlow", "Python", "Computer Vision"], "title": "Software Developer at Untether.AI", "url": "/experience/untether", "video": null}, "camping": {"name": "camping", "priority": 5, "title": "Camping Trips", "blurb": "One of my more intense hobbies is camping, specifically canoe tripping.", "intro": "My family is outdoors oriented, and we've been involved in the Scouts Canada program for many years. I've done winter camps and backpacking camps, but the most impressive trips are whitewater canoe trips with my family.", "custom": [{"header": "Most Recent Trip", "body": "My most recent trip was a 14-Day canoe trip down the lower half of the Missinaibi River. I took my drone on the trip, and got some pretty nice shots. See the video below."}], "dataEnd": "Present", "dateStart": "Super Early", "gallery": [{"height": "2988", "src": "1.jpg", "width": "5312"}, {"height": "3648", "src": "3.jpg", "width": "2736"}, {"height": "2448", "src": "2.jpg", "width": "2448"}, {"height": "960", "src": "4.jpg", "width": "540"}], "img": "0.jpg", "learned": ["Quick decision-making and communication skills", "Logistics and planning", "Teamwork and perseverance"], "tags": ["Planning", "Management", "Communication", "White Water Canoeing"], "url": "/experience/camping", "video": "https://www.youtube.com/embed/yUcVznxGcTA"}}}